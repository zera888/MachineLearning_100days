{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請觀看李宏毅教授以神奇寶貝進化 CP 值預測的範例，解說何謂機器學習與過擬合。並回答以下問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[youtube](https://www.youtube.com/watch?v=fegAeph9UaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型的泛化能力 (generalization) 是指什麼？ \n",
    "### 2. 分類問題與回歸問題分別可用的目標函數有哪些？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 模型的泛化能力 (generalization) 是指什麼？\n",
    "\n",
    "Answer： 何謂「泛化（Generalization）」，意思是我們的設計模組可以應對未來的數據，也就是可以被廣泛使用卻仍在我們模型範圍內，也就是他的適應性很好\n",
    "        ，這就是泛化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. 分類問題與回歸問題分別可用的目標函數有哪些？\n",
    "Answer：\n",
    "        分類:交叉熵損失(cross-entropy)  \n",
    "        回歸:均方方差 (mean square error，MSE)、均方誤差 (MSE) 是最常用的回歸損失函數，計算方法是求預測值與真實值之間距離的平方和。  \n",
    "        平均絕對值誤差(Mean absolute error，MAE)、平均絕對誤差（MAE）是另一種用於回歸模型的損失函數。MAE 是目標值和預測值之差的絕對值之和。  \n",
    "        Huber loss、Huber 損失，平滑的平均絕對誤差。本質上，Huber 損失是絕對誤差，只是在誤差很小時，就變為平方誤差。誤差降到多小時變為二次誤差\n",
    "        由超參數 δ（delta）來控制。 \n",
    "        Los cosh loss、Log-cosh 是另一種應用於回歸問題中的，且比 L2 更平滑的的損失函數。它的計算方式是預測誤差的雙曲餘弦的對數。  \n",
    "        Quantile loss。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
