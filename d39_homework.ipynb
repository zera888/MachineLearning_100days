{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "\n",
    "Answer ：\n",
    "            在《線性回歸（Linear Regression）》中提到過，當使用最小二乘法計算線性回歸模型參數的時候，如果資料集合矩陣（也叫做設\n",
    "            計矩陣(design matrix)）X，存在多重共線性，那麼最小二乘法對輸入變數中的雜訊非常的敏感，其解會極為不穩定。為了解決這\n",
    "            個問題，就有了這一節脊回歸（Ridge Regression ）。\n",
    "            當設計矩陣X存在多重共線性的時候（數學上稱為病態矩陣），最小二乘法求得的參數w在數值上會非常的大，而一般的線性回歸其\n",
    "            模型是 y=wTx ，顯然，就是因為w在數值上非常的大，所以，如果輸入變數x有一個微小的變動，其反應在輸出結果上也會變得非常\n",
    "            大，這就是對輸入變數總的雜訊非常敏感的原因。\n",
    "            如果能限制參數w的增長，使w不會變得特別大，那麼模型對輸入w中雜訊的敏感度就會降低。這就是脊回歸和套索回歸\n",
    "            （Ridge Regression and Lasso Regrission）的基本思想。\n",
    "            為了限制模型參數w的數值大小，就在模型原來的目標函數上加上一個懲罰項，這個過程叫做正則化（Regularization）。\n",
    "            如果懲罰項是參數的l2範數，就是脊回歸(Ridge Regression)\n",
    "            如果懲罰項是參數的l1范數，就是套索回歸（Lasso Regrission）\n",
    "            正則化同時也是防止過擬合有效的手段，這在“多項式回歸”中有詳細的說明。\n",
    "            \n",
    "            特徵選擇(Feature selection)是從所有特徵中選出一組相關的特徵(特徵子集)，而去掉無關的特徵，而這個過程可以看做是一個降\n",
    "            維過程。\n",
    "\n",
    "            嵌入類方法(Embedded)包括了所有構建模型過程中用到的特徵選擇技術。這類方法的典範是構建線性模型的LASSO方法。該方法給回\n",
    "            歸係數加入了L1懲罰，導致其中的許多參數趨於零。任何回歸係數不為零的特徵都會被LASSO演算法「選中」。 LASSO的改良演算法\n",
    "            有Bolasso和FeaLect。 Bolasso改進了樣本的初始過程。 FeaLect根據回歸係數組合分析給所有特徵打分。另外一個流行的做法是\n",
    "            遞歸特徵消除（Recursive Feature Elimination）演算法，通常用於支援向量機，通過反覆構建同一個模型移除低權重的特徵。這\n",
    "            些方法的計算複雜度往往在過濾類和包裝類之間。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.  當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "\n",
    "Answer  ：\n",
    "            進行迴歸分析時，當遭遇以下幾種情況時，可能要注意迴歸模式中是否存在多元共線性（multi-collinearity）的問題：（1）共線\n",
    "            性指標超過標準（含容忍度tolerance、變異膨脹因子VIF、條件指標CI）（2）迴歸係數的方向性與相關係數相反（3）解釋力R平方\n",
    "            過高，但個別變項的係數未達顯著水準。\n",
    "            \n",
    "            脊迴歸方程式有以下幾點特性：（1）當偏化常數c=0時，即為原先的不偏估計式；（2）當c值由0開始微些增加時，此時估計參數\n",
    "            bR的改變幅度最大，甚至發生係數正負值的改變，隨著c值再增加時，迴歸係數bR的改變幅度會不斷變小，且迴歸係數bR越趨近於0；\n",
    "            （3）個別VIF和估計參數bR的情形一樣，當c值由0開始些微增加時，VIF值會迅速下降，隨著c值再增加時，VIF的下降幅度會不斷減\n",
    "            少；（4）當偏化常數c不斷增加時，迴歸模式的解釋力R2會不斷降低。\n",
    "\n",
    "            根據以上幾點特性，在進行脊迴歸分析時，會建議從脊跡選擇一個迴歸係數趨於穩定（變項迴歸係數的正負值合理），VIF夠小（盡\n",
    "            量維持在1~2之間），且c值盡量越小越好。 \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
